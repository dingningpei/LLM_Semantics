{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au9Zs43Cd5jq",
        "outputId": "31cd2aa7-bda8-4216-e2d8-6f7858a728fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Prompt_Semantic\n",
            "/content/drive/MyDrive/Prompt_Semantic\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx4O3lWeeFFq",
        "outputId": "04370fba-5995-4264-e8ba-13ad65eeaaee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers (from -r requirements.txt (line 4))\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-uj5bje7b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-uj5bje7b\n",
            "  Resolved https://github.com/huggingface/transformers to commit 746104ba6f0514159d58dc2fb09c887d0e9d4863\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rdflib (from -r requirements.txt (line 1))\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.1.4)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.4.2)\n",
            "Collecting gguf (from -r requirements.txt (line 5))\n",
            "  Downloading gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting isodate<0.7.0,>=0.6.0 (from rdflib->-r requirements.txt (line 1))\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0->-r requirements.txt (line 4)) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0->-r requirements.txt (line 4)) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0->-r requirements.txt (line 4)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0->-r requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0->-r requirements.txt (line 4)) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0->-r requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0->-r requirements.txt (line 4)) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0->-r requirements.txt (line 4)) (0.4.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0->-r requirements.txt (line 4)) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0->-r requirements.txt (line 4)) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0->-r requirements.txt (line 4)) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0->-r requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0->-r requirements.txt (line 4)) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0->-r requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0->-r requirements.txt (line 4)) (2024.7.4)\n",
            "Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.10.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9614041 sha256=b8647a66dc2c6b2de8b74ab5227defb6b54b896c773214cff3e2be98f180ecae\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q73sdd4d/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: isodate, gguf, rdflib, transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "Successfully installed gguf-0.10.0 isodate-0.6.1 rdflib-7.0.0 transformers-4.45.0.dev0\n"
          ]
        }
      ],
      "source": [
        "# Set up the environment\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oOkDqwqUuPF0"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers accelerate bitsandbytes>0.37.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q_5ebcGj2LR"
      },
      "outputs": [],
      "source": [
        "# !python main.py \\\n",
        "#   --train_date_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hTCpv4PL_2zN"
      },
      "outputs": [],
      "source": [
        "train_path = \"/content/drive/MyDrive/Prompt_Semantic/Data/karma-version/sources/\"\n",
        "models_path = \"/content/drive/MyDrive/Prompt_Semantic/Data/models-triples/\"\n",
        "ontology_path = \"/content/drive/MyDrive/Prompt_Semantic/Data/ontology/Ontologies.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvMm2bjIA9gi",
        "outputId": "42fcf542-3613-4686-e8ee-81834dc647af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from [0, 1] files\n",
            "Loading data from /content/drive/MyDrive/Prompt_Semantic/Data/karma-version/sources/s01-cb.json and /content/drive/MyDrive/Prompt_Semantic/Data/karma-version/models-json/s01-cb-model.json\n",
            "Loading data from /content/drive/MyDrive/Prompt_Semantic/Data/karma-version/sources/s02-dma.json and /content/drive/MyDrive/Prompt_Semantic/Data/karma-version/models-json/s02-dma-model.json\n",
            "Loading data from [2] files\n",
            "Loading data from /content/drive/MyDrive/Prompt_Semantic/Data/karma-version/sources/s03-ima-artists.json and /content/drive/MyDrive/Prompt_Semantic/Data/karma-version/models-json/s03-ima-artists-model.json\n"
          ]
        }
      ],
      "source": [
        "from data_loader import generate_data\n",
        "from prompt_generator import load_ontologies, generate_prompt_without_semantic_typing\n",
        "from instruct_prompt_generator import load_ontologies, generate_instruct_prompt\n",
        "train_data, test_data = generate_data(train_path, models_path,  6, 3)\n",
        "ontology = load_ontologies(ontology_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "#The system can add number to the semantic types to distinguish the different semantic type nodes with the same name. For example: crm:E42_Identifier1, crm:E42_Identifier2, crm:E42_Identifier3 represent different semantic type nodes.\n",
        "def generate_prompt_chain(train_data, test_data, model, ontologies, with_inst=False):\n",
        "    \"\"\"Generate a prompt chain for the LLM based on the table data and ontologies.\"\"\"\n",
        "    system_prompt = \"You are a helpful assistant that can generate semantic graphs for tables based on the ontologies given(delimited by '<Ontology>' and '</Ontology>') and the table data given(delimited by '<Table>' and '</Table>'). \\\n",
        "        In the Ontology part, nodes is delimited by '<Nodes>' and '</Nodes>', properties is delimited by '<Properties>' and '</Properties>', and potential triples is delimited by '<PotentialTriples>' and '</PotentialTriples>'. The symbol '->' in the ontology is used to represent the 'is parent class of' relation. \\\n",
        "        In the Table part, each table is a list of dictionaries, where each dictionary represents a row of the table, and the key-value pairs are the column names and their corresponding values.\"\n",
        "    nodes = []\n",
        "    properties = []\n",
        "    potential_triples = []\n",
        "    for ontology_name, ontology_data in ontologies.items():\n",
        "        for node in ontology_data['Nodes']:\n",
        "            nodes.append(node)\n",
        "        for prop in ontology_data['Properties']:\n",
        "            properties.append(prop)\n",
        "        for triples in ontology_data['Potential triples']:\n",
        "            potential_triples.append(triples.replace(\"<\", \"[\").replace(\">\", \"]\"))\n",
        "    system_prompt += f\"<Ontology> <Nodes> {nodes}</Nodes> \\n <Properties> {properties}</Properties> \\n <PotentialTriples> {potential_triples}</PotentialTriples></Ontology> \\n\"\n",
        "    system_prompt += \"You will solve the task by two steps: Step1(delimited by '<Step1>' and '</Step1>') Identify the appropriate semantic type and property for each column in the table. Step2(delimited by '<Step2>' and '</Step2>') Generate a semantic graph for the table. The solution are delimited by '<Solution>' and '</Solution>'.\\n\\\n",
        "The examples of tasks are delimited by '<Examples>' and '</Examples>'. \\n\"\n",
        "    examples = \"<Examples>\"\n",
        "    for example_data in train_data:\n",
        "        examples += f\"<Table> {example_data['table']} </Table>\\n\"\n",
        "        examples += \"<Solution>\\n\"\n",
        "        examples += f\"<Step1> {example_data['semantic_graph']['SetSemanticType']} </Step1>\\n\"\n",
        "        examples += f\"<Step2> {example_data['semantic_graph']['SetInternalLink']} </Step2>\\n\"\n",
        "        examples += \"</Solution>\"\n",
        "    examples += \"</Examples>\"\n",
        "    system_prompt += examples\n",
        "# 2. Extract the column names and their corresponding row values from the lists of dictionaries. \\n\\\n",
        "# 4. Rethink the internal links and the list of triples of semantic types and properties, change the unlogical part of internal links and list of triples of semantic types and properties. \\n\\\n",
        "    chain_prompt_1 = \"Step1 SetSemanticType: \\n\\\n",
        "The system will first identify the appropriate semantic type and property for each column in the table. \\n\\\n",
        "Solution Steps: \\n\\\n",
        "1. Begin the response with 'Let's think step by step'. \\n\\\n",
        "2. Following the reasoning steps, search the potential triples whose subject is start with  in the ontology to find the appropriate semantic types for each column in the table by reasoning the column name and the row values. All columns must be mapped to the Ontology. \\\n",
        "2. Following with reasoning steps, identify the appropriate semantic types from nodes in the ontology for each column in the table by reasoning the column name and the row values. All columns must be mapped to the Ontology. \\\n",
        "ONLY if all of the row values in the column are <Empty>, the system will take the column name into consideration to find the appropriate semantic types from nodes in the ontology. \\n\\\n",
        "3. Find the appropriate properties from properties in the ontology to link the columns and the semantic types by reasoning the column names, the row values and the semantic types. \\n\\\n",
        "4. The system will output the semantic types and properties in the format of a list of triples, where each triple has the form (subject, predicate, object). The subject is the semantic type, the predicate is the property, and the object is the column in the table.\\\n",
        "Please output the list of triples using <Step1> </Step1> tags in the format of in the Step1 of examples. For example: <Step1> [(semantic_type, property, column), (semantic_type, property, column), ...] </Step1>. \"\n",
        "    table  =f\"<Table> {test_data} </Table>\"\n",
        "    chain_prompt_1 = chain_prompt_1 + table\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": chain_prompt_1}\n",
        "    ]\n",
        "    response = model.chat.completions.create(\n",
        "    model=\"deepseek-chat\",\n",
        "    # model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "        stream=False\n",
        "    )\n",
        "    response_content = response.choices[0].message.content\n",
        "    messages.append({\"role\": \"assistant\", \"content\": response_content})\n",
        "    set_semantic_type = response_content.split(\"<Step1>\")[1].split(\"</Step1>\")[0]\n",
        "    print(response_content)\n",
        "#3. Review internal links and the list of triples of semantic types and properties, find the same semantic type which link to different internal nodes or the same internal node linked to different internal nodes. add a number to these semantic types and internal nodes to distinguish the different the nodes in the graph with the same name. \\n\\\n",
        "    chain_prompt_2 = \" Let us do Step2 given the table and Step1. \\n\\\n",
        "Step2 Problem Statement: \\n\\\n",
        "Given the list of triples of semantic types of the table (delimited by '<Step1>' and '</Step1>') and the original table, generate a semantic graph for the table. The semantic graph is a graph with nodes and edges. \\\n",
        "The nodes are the semantic types or internal nodes in the ontology, and the edges are the properties that link between the nodes. Hint: The graph is a tree structure.\\n\\\n",
        "Solution: \\n\\\n",
        "1. Begin the response with 'Let's think step by step'. \\n\\\n",
        "2. Find the main root of the graph.  \\n\\\n",
        "2. Following with reasoning steps, establish internal links between the semantic types based on the properties defined in the ontology and the nodes in the ontology. \\n\\\n",
        "3. Review the internal links and the list of triples of semantic types and properties to find the semantic types which is isolated from the graph. Go back to Step1 to find the appropriate semantic types for these isolated semantic types to ensure all semantic types are connected in the graph and after that update the step1 solution. Hint: The graph is a tree structure. \\n\\\n",
        "4. Output the final internal links using <Step2> </Step2> tag in the format of in the Step2 of examples. For example: <Step2> [(internal_node, property, internal_node), (internal_node, property, semantic_type), (internal_node, property, internal_node), ...] </Step2>.\\n\"\n",
        "    chain_prompt_2 = chain_prompt_2 + f\"<Step1> {set_semantic_type} </Step1>\\n\" + table\n",
        "    messages2 = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": chain_prompt_2}\n",
        "    ]\n",
        "    response = model.chat.completions.create(\n",
        "        model=\"deepseek-chat\",\n",
        "        messages=messages2,\n",
        "        stream=False,\n",
        "    )\n",
        "    response_content = response.choices[0].message.content\n",
        "    messages2.append({\"role\": \"assistant\", \"content\": response_content})\n",
        "    return response_content, messages2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "model = OpenAI(api_key=\"\")\n",
        "res, mesg = generate_prompt_chain(train_data, test_data[0]['table'], model, ontology, True)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "####Draft below###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "akwNZrl3kyBN"
      },
      "outputs": [],
      "source": [
        "for i, data in enumerate(test_data[:1]):\n",
        "    message.append({\"role\": \"user\", \"content\": f\"Table: {data['table']}. Please provide a step-by-step breakdown you do and detailed information about it.\"})   \n",
        "    # prompt += f\". The semantic graph of the table in the strict format of given examples above:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d-g0lI18QqG7"
      },
      "outputs": [],
      "source": [
        "text_file = open(\"Output.txt\", \"w\")\n",
        "\n",
        "text_file.write(str(message))\n",
        "\n",
        "text_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yjdpp2fKgBLY"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "model = OpenAI(api_key=\"\", base_url=\"https://api.deepseek.com\")\n",
        "\n",
        "response = model.chat.completions.create(\n",
        "    model=\"deepseek-chat\",\n",
        "    messages=message,\n",
        "    stream=False,\n",
        ")\n",
        "response_content = response.choices[0].message.content\n",
        "text_file = open(\"response.txt\", \"w\")\n",
        "\n",
        "text_file.write(response_content)\n",
        "\n",
        "text_file.close()\n",
        "print(f\"Messages Round 1: {response.choices[0].message.content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "message.append({\"role\": \"user\", \"content\": \"I don't want to seperate the column into different SemanticType and do not ignore the columns whose value  <Empty>.\"})\n",
        "response = model.chat.completions.create(\n",
        "    model=\"deepseek-chat\",\n",
        "    messages=message\n",
        ")\n",
        "\n",
        "message.append(response.choices[0].message)\n",
        "print(f\"Messages Round 2: {response.choices[0].message.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "message.append(response.choices[0].message)\n",
        "message.append({\"role\": \"user\", \"content\": \"Generate the semantic graph.\"})\n",
        "response = model.chat.completions.create(\n",
        "    model=\"deepseek-chat\",\n",
        "    messages=message\n",
        ")\n",
        "message.append(response.choices[0].message)\n",
        "print(f\"Messages Round 2: {response.choices[0].message.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6WWVGq5tQre9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Positives: 17\n",
            "Intersection: {('crm:E52_Time-Span1', 'rdfs:label', 'Date'), ('crm:E33_Linguistic_Object4', 'rdf:value', 'Description'), ('crm:E82_Actor_Appellation1', 'rdf:value', 'Who'), ('crm:E33_Linguistic_Object2', 'rdf:value', 'Dimensions'), ('crm:E42_Identifier1', 'rdf:value', 'id'), ('crm:E52_Time-Span1', 'crm:P82a_begin_of_the_begin', 'date_earliest'), ('crm:E17_Type_Assignment1', 'karma:classLink', 'type_assignment_uri'), ('crm:E42_Identifier2', 'rdf:value', 'Accession Number'), ('crm:E38_Image1', 'karma:classLink', 'image'), ('crm:E12_Production1', 'karma:classLink', 'production_uri'), ('crm:E53_Place1', 'rdfs:label', 'Geography'), ('crm:E33_Linguistic_Object3', 'rdf:value', 'Credit Line'), ('crm:E33_Linguistic_Object5', 'rdf:value', 'Provenance'), ('crm:E22_Man-Made_Object1', 'karma:classLink', 'object_uri'), ('crm:E33_Linguistic_Object1', 'rdf:value', 'Medium'), ('crm:E52_Time-Span1', 'crm:P82b_end_of_the_end', 'date_latest'), ('crm:E39_Actor1', 'karma:classLink', 'artist_uri')}\n",
            "Precision: 0.9444444444444444\n",
            "Recall: 0.8947368421052632\n",
            "Differences between the triples:\n",
            "['crm:E17_Type_Assignment2', 'crm:P42_assigned', 'What']\n",
            "['crm:E17_Type_Assignment1', 'crm:P42_assigned', 'Classification']\n",
            "['crm:E55_Type1', 'rdfs:label', 'Classification']\n",
            "['skos:Concept1', 'skos:prefLabel', 'What']\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "def mean_reciprocal_rank(predicted, gold):\n",
        "    ranks = []\n",
        "    for item in gold:\n",
        "        if item in predicted:\n",
        "            rank = predicted.index(item) + 1\n",
        "            ranks.append(1 / rank)\n",
        "    return sum(ranks) / len(gold) if ranks else 0\n",
        "\n",
        "\n",
        "    from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "def calculate_precision_recall(gold_semantics, predicted_semantics):\n",
        "    # Convert lists to sets for comparison\n",
        "    gold_set = set(tuple(item) for item in gold_semantics)\n",
        "    predicted_set = set(tuple(item) for item in predicted_semantics)\n",
        "\n",
        "    # Calculate true positives, false positives, and false negatives\n",
        "    true_positives = len(gold_set.intersection(predicted_set))\n",
        "    false_positives = len(predicted_set - gold_set)\n",
        "    false_negatives = len(gold_set - predicted_set)\n",
        "    print(f\"True Positives: {true_positives}\")\n",
        "    print(f\"Intersection: {gold_set.intersection(predicted_set)}\")\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision = true_positives / (len(predicted_set)  - 1) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / len(gold_set) if (true_positives + false_negatives) > 0 else 0\n",
        "    return precision, recall\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def find_differences(list1, list2):\n",
        "    set1 = set(tuple(item) for item in list1)\n",
        "    set2 = set(tuple(item) for item in list2)\n",
        "    differences = set1.symmetric_difference(set2)\n",
        "    return [list(item) for item in differences]\n",
        "\n",
        "# Load the JSON files\n",
        "gold_semantics= load_json('Data/models-triples/s05-met-model.json')['semantic_triples']\n",
        "predicted_semantics = load_json('s03_predict.json')['SetSemanticType']  # Replace with the actual path to the other file\n",
        "\n",
        "precision, recall = calculate_precision_recall(gold_semantics, predicted_semantics)\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "# # Calculate MRR\n",
        "# mrr = mean_reciprocal_rank(predicted_semantics, gold_semantics)\n",
        "# print(f\"Mean Reciprocal Rank (MRR): {mrr}\")\n",
        "\n",
        "# Find differences\n",
        "differences = find_differences(predicted_semantics, gold_semantics)\n",
        "print(\"Differences between the triples:\")\n",
        "for diff in differences:\n",
        "    print(diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['crm:E22_Man-Made_Object1',\n",
              "  'crm:P108i_was_produced_by',\n",
              "  'crm:E12_Production1'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P41i_was_classified_by',\n",
              "  'crm:E17_Type_Assignment1'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P67i_is_referred_to_by',\n",
              "  'crm:E33_Linguistic_Object3'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P67i_is_referred_to_by',\n",
              "  'crm:E33_Linguistic_Object4'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P67i_is_referred_to_by',\n",
              "  'crm:E33_Linguistic_Object5'],\n",
              " ['crm:E22_Man-Made_Object1', 'crm:P102_has_title', 'crm:E35_Title1'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P138i_has_representation',\n",
              "  'crm:E38_Image1'],\n",
              " ['crm:E12_Production1', 'crm:P14_carried_out_by', 'crm:E39_Actor1'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P1_is_identified_by',\n",
              "  'crm:E42_Identifier1'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P1_is_identified_by',\n",
              "  'crm:E42_Identifier2'],\n",
              " ['crm:E12_Production1', 'crm:P4_has_time-span', 'crm:E52_Time-Span1'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P53_has_former_or_current_location',\n",
              "  'crm:E53_Place1'],\n",
              " ['crm:E22_Man-Made_Object1', 'crm:P43_has_dimension', 'crm:E54_Dimension1'],\n",
              " ['crm:E17_Type_Assignment1', 'crm:P42_assigned', 'crm:E55_Type1'],\n",
              " ['crm:E22_Man-Made_Object1', 'crm:P45_consists_of', 'crm:E57_Material1'],\n",
              " ['crm:E39_Actor1', 'crm:P131_is_identified_by', 'crm:E82_Actor_Appellation1'],\n",
              " ['crm:E22_Man-Made_Object1', 'dcterms:subject', 'skos:Concept1']]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_semantics.sort(key=lambda x: x[2])\n",
        "gold_semantics.sort(key=lambda x: x[2]  )\n",
        "predicted_semantics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['crm:E22_Man-Made_Object1',\n",
              "  'crm:P108i_was_produced_by',\n",
              "  'crm:E12_Production1'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P41i_was_classified_by',\n",
              "  'crm:E17_Type_Assignment1'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P67i_is_referred_to_by',\n",
              "  'crm:E33_Linguistic_Object1'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P67i_is_referred_to_by',\n",
              "  'crm:E33_Linguistic_Object2'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P67i_is_referred_to_by',\n",
              "  'crm:E33_Linguistic_Object3'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P67i_is_referred_to_by',\n",
              "  'crm:E33_Linguistic_Object4'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P67i_is_referred_to_by',\n",
              "  'crm:E33_Linguistic_Object5'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P138i_has_representation',\n",
              "  'crm:E38_Image1'],\n",
              " ['crm:E12_Production1', 'crm:P14_carried_out_by', 'crm:E39_Actor1'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P1_is_identified_by',\n",
              "  'crm:E42_Identifier1'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P1_is_identified_by',\n",
              "  'crm:E42_Identifier2'],\n",
              " ['crm:E12_Production1', 'crm:P4_has_time-span', 'crm:E52_Time-Span1'],\n",
              " ['crm:E22_Man-Made_Object1',\n",
              "  'crm:P55_has_current_location',\n",
              "  'crm:E53_Place1'],\n",
              " ['crm:E17_Type_Assignment1', 'crm:P42_assigned', 'crm:E55_Type1'],\n",
              " ['crm:E39_Actor1', 'crm:P131_is_identified_by', 'crm:E82_Actor_Appellation1'],\n",
              " ['crm:E22_Man-Made_Object1', 'dcterms:subject', 'skos:Concept1']]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gold_semantics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    {\n",
        "        \"id\":10000831,\n",
        "        \"image\":\"http:\\/\\/images.metmuseum.org\\/CRDImages\\/ad\\/web-large\\/DP104414.jpg\",\n",
        "        \"Date\":\"1700-1710\",\n",
        "        \"date_latest\":\"\",\n",
        "        \"date_earliest\":\"\",\n",
        "        \"Geography\":\"Mid-Atlantic, New York City, New York, United States\",\n",
        "        \"Medium\":\"Silver\",\n",
        "        \"Dimensions\":\"\\n\",\n",
        "        \"Classification\":\"Silver\",\n",
        "        \"Credit Line\":\"Samuel D. Lee Fund, 1938\",\n",
        "        \"Accession Number\":\"38.63\",\n",
        "        \"object_uri\":\"http:\\/\\/www.metmuseum.org\\/object\\/38.63\",\n",
        "        \"type_assignment_uri\":\"http:\\/\\/www.metmuseum.org\\/object\\/38.63\\/work_type\",\n",
        "        \"production_uri\":\"http:\\/\\/www.metmuseum.org\\/object\\/38.63\\/production\",\n",
        "        \"Who\":\"Cornelius Kierstede\",\n",
        "        \"artist_uri\":\"http:\\/\\/www.metmuseum.org\\/person\\/cornelius-kierstede\",\n",
        "        \"What\":[\n",
        "            \"Metal\",\n",
        "            \"Silver\",\n",
        "            \"Bowls\"\n",
        "        ],\n",
        "        \"Description\":\"Derived from Dutch, Scandinavian, and English sources, brandywine bowls were used on festive occasions such as the kindermaal, when women gathered to welcome a newborn child. Following Dutch custom, a bowl would be filled with raisins and brandy and passed from guest to guest. Here, boldly chased tulips create a sumptuous effect. The bowl belonged to Theunis Jacobsen Quick, a wealthy baker, and his wife, Vroutje 4516 Janse Haring.\",\n",
        "        \"Provenance\":\"\\n\"\n",
        "    },"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3c76bdb8ba584288b82864971dd129f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a2d6f9b679642ca86295b9e6d188e00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e53c82f871749c7aaeb2b7943b52eaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "596b395997c74cd0995704c6d05892f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e7331ecba68460ba6f88192fdecb3ce",
            "max": 3230186400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8947cbe314a04b25abacc93781a44399",
            "value": 3230186400
          }
        },
        "5e7331ecba68460ba6f88192fdecb3ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "871494e5820c4a18aeda67331488a6a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a2d6f9b679642ca86295b9e6d188e00",
            "placeholder": "​",
            "style": "IPY_MODEL_c68a95d6fe6c4124a27a90c73ed08f81",
            "value": "(…)a-3.1-Minitron-4B-Width-Base-Q5_K_M.gguf: 100%"
          }
        },
        "8947cbe314a04b25abacc93781a44399": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1466e3fc939432cb256a08a95565689": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c68a95d6fe6c4124a27a90c73ed08f81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e71e5a3e33684309bc28113f18afc4f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_871494e5820c4a18aeda67331488a6a3",
              "IPY_MODEL_596b395997c74cd0995704c6d05892f1",
              "IPY_MODEL_fe52dd50f1dd4273a11bbb0d6afc1abf"
            ],
            "layout": "IPY_MODEL_3c76bdb8ba584288b82864971dd129f1"
          }
        },
        "fe52dd50f1dd4273a11bbb0d6afc1abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1466e3fc939432cb256a08a95565689",
            "placeholder": "​",
            "style": "IPY_MODEL_4e53c82f871749c7aaeb2b7943b52eaa",
            "value": " 3.23G/3.23G [02:14&lt;00:00, 24.2MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
